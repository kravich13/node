# Streams

- [Streams](#streams)
	- [Intro](#intro)
	- [Reading Modes](#reading-modes)
		- [`readable.readableFlowing === null|false|true`](#readablereadableflowing--nullfalsetrue)
	- [Events](#events)
		- [`readable.on(`**`'data'`**`, callback)`](#readableondata-callback)
		- [`readable.on('`**`readable'`**`)`](#readableonreadable)
		- [`writable.on(`**`'close'`**`)`](#writableonclose)
		- [`writable.on(`**`'drain'`**`)`](#writableondrain)
		- [`writable.on(`**`'finish'`**`)`](#writableonfinish)
		- [`writable.on(`**`'pipe'`**`)`](#writableonpipe)
		- [`writable.on(`**`'unpipe'`**`)`](#writableonunpipe)
	- [Methods](#methods)
		- [`readable.`**`read`** `([size])`](#readableread-size)
		- [`readable.`**`pipe`** `(destination[, options])`](#readablepipe-destination-options)
		- [`stream.`**`pipeline`**`(source[, ...transforms], destination, callback)`](#streampipelinesource-transforms-destination-callback)
	- [Create](#create)
		- [Readable](#readable)
		- [Writable](#writable)

***

## Intro

Stream is a concept that allows **processing data by little pieces** or **chunks**.

There are 4 types of streams in node:

1. `Readable` - read (**req**)
2. `Writable` - write (**res**)
3. `Duplex` - both
4. `Transform` - a variation of duplex allowing to change data

Example: send file to the client **at once** and using **streams and chunks**

```js
// 1 - simply send a file
async function sendFile(req, req, next) {
	fs.readFile('01. Dreaming Wide Awake.mp3', (err, data) => {
		if (err) {
			throw err
		}
		res.end(data)
	})
}

// 2 - send a file using streams and chunks
async function sendFile(req, req, next) {
	const fileStream = fs.createReadStream('path to file')
	res.contentType('application/pdf')
	fileStream.pipe(res)
}
```

***


## Reading Modes

There are 2 modes for `Readable` streams: 

- **flowing mode**: data is read automatically and poured as quickly as possible.
- **paused mode**: `stream.read()` mush be called explicitly to read chunks of data from the stream. 

`Readable` streams begin in **paused** mode but can be switched to the **flowing** one by:

- adding `data` event listener
- `stream.resume()`
- `stream.pipe()`

Switch back to **paused**:

- if there're no pipe destinations - `stream.pause()`
- if there're pipe destinations - remove them (`stream.unpipe()`)

**TLDR**: use the `readable` event and the `.read()` method to read the data.

***

### `readable.readableFlowing === null|false|true`

When `null` - no reading mechanism is provided, so data is not being read. 

***


## Events

**NB**: The default chunk size is **2 kB**.

### `readable.on(`**`'data'`**`, callback)`

The old way ot reading data. When the stream is in **flowing mode** (e.g. when the `data` listener is attached), chunks of data are passes as soon as possible. 

```js
const readable = getReadableStreamSomehow()
readable.on('data', (chunk) => {
	console.log(`Received ${chunk.length} bytes of data.`)
})
readable.on('end', () => {
	console.log('There will be no more data.')
})

/* ...
Received 65536 bytes of data.
Received 36595 bytes of data.
There will be no more data. */

```

Don't use it, use **`readable + .read()`** instead.

***


### `readable.on('`**`readable'`**`)`

The difference with `'data'` is that the stream is **not flowing**. Instead, we control the flow calling `stream.read()` explicitly.

Emitted when there is data available to be read from the stream (`stream.read()` will return **data** in this case). Will also be emitted at the end, just before the `end` event (`stream.read()` will return `null`).

```js
const readable = getReadableStreamSomehow()
readable.on('readable', function () {
	// There is some data to read now.
	let chunk

	while ((chunk = this.read()) !== null) {
		console.log(chunk)
	}
})
readable.on('end', function () {
	console.log('Stream finished')
})
```

If both `'readable'` and `'data'` are used at the same time, `'readable'` takes precedence in controlling the flow, i.e. `'data'` will be emitted only when `stream.read()` is called. The `readableFlowing` property would become `false`. If there are `'data'` listeners when `'readable'` is removed, the stream will start flowing, i.e. `'data'` events will be emitted without calling `.resume()`.

***


### `writable.on(`**`'close'`**`)`

Emitted when the stream and any of its underlying resources (e.g. a file descriptor) have been closed. The event indicates that **no more events will be emitted**, and **no further computation will occur**.

A `Writable` stream will always emit the `'close'` event if it is created with the `emitClose` option.

***


### `writable.on(`**`'drain'`**`)`

If a call to `stream.write(chunk)` returns `false`, the `'drain'` event will be emitted when it is appropriate to resume writing data to the stream.



TO ADD EXAMPLE


***


### `writable.on(`**`'finish'`**`)`

Emitted after the `stream.end()` method has been called, and all data has been flushed to the underlying system.



TO ADD EXAMPLE


***


### `writable.on(`**`'pipe'`**`)`

Emitted when the `stream.pipe()` method is called on a `Readable` stream, adding this `writable` to its set of destinations.


TO ADD EXAMPLE



***


### `writable.on(`**`'unpipe'`**`)`

Emitted when the `stream.unpipe()` method is called on a `Readable` stream, removing this `Writable` from its set of destinations.

Also emitted in case this `Writable` stream emits an **error** when a `Readable` stream pipes into it.


TO ADD EXAMPLE



***



## Methods

### `readable.`**`read`** `([size])`

Returns data from the internal buffer. If no data available, `return null`. 

If `readable.setEncoding()` was used to set the default encoding, then the chunk of data will be passed as a `string`, otherwise - as a `Buffer`.

`size` specifies how many bytes to read (should be **<= 1 GB**). If none is set, all of the data will be returned.

When reading a **large file** `.read()` may return `null`, having consumed all buffered content so far, but there is still more data to come **not yet buffered**. In this case a new `'readable'` event will be emitted when there is more data in the buffer. Finally the `'end'` event will be emitted when there is no more data to come. 

Therefore to read a file's whole contents from a readable, it is necessary to collect chunks across **multiple** `'readable'` events:

```js
const chunks = []

readable.on('readable', () => {
	let chunk
	while((chunk = readable.read()) !== null){
		chunks.push(chunk)
	}
})

readable.on('end', () => {
	const content = chunks.join('')
})
```

***


### `readable.`**`pipe`** `(destination[, options])`

Attaches a `Writable` stream to the `Readable`, causing it to switch automatically into **flowing mode** and push all of its data to the attached `Writable`.

It is also possible to attach multiple `Writable` streams to a single `Readable` one. 

Returns a reference to the **destination** stream making it possible to **chain** pipes if it is a `Duplex` or a `Transform` stream. 

```js
const r = fs.createReadStream('01. Dreaming Wide Awake.mp3')

const w1 = fs.createWriteStream('clone1.mp3')
const w2 = fs.createWriteStream('clone2.mp3')

r.pipe(w1)
r.pipe(w2)
```

***

**There is a problem with pipes**. If the `Readable` stream emits an **error** during processing, the `Writable` destination is **not closed** automatically: you'll have to manually close each stream in order to prevent memory leaks.

**Don't ever do this:**

```js
http.createServer((req, res) => {
	fs.createReadStream(__filename).pipe(res)
})
```

If the client closes connection before the transfer is over, or any other error occurs, the stream will **not end automatically**! It will hang forever.

The better way to pipe something is through the `pipeline`.

***


### `stream.`**`pipeline`**`(source[, ...transforms], destination, callback)`

A newer and better way to do pipes. 

```js
const { pipeline } = require('stream')

const server = http.createServer((req, res) => {
	// automatically destroy `res` if there's an error
	pipeline(fs.createReadStream(__filename), pipe(res), err => {
		if(err){
			console.log(err)
		}
	})
})
```

Example with archiving:

```js
const {	pipeline } = require('stream')
const fs = require('fs')
const zlib = require('zlib')

// Use the pipeline API to easily pipe a series of streams
// together and get notified when the pipeline is fully done.

// A pipeline to gzip a potentially huge tar file efficiently:

pipeline(
	fs.createReadStream('archive.tar'),
	zlib.createGzip(),
	fs.createWriteStream('archive.tar.gz'),
	(err) => {
		if (err) {
			console.error('Pipeline failed.', err)
		} else {
			console.log('Pipeline succeeded.')
		}
	}
)
```

You can primisify it as well:

```js
async function run() {
	await pipeline(
		fs.createReadStream('archive.tar'),
		zlib.createGzip(),
		fs.createWriteStream('archive.tar.gz')
	)
	console.log('Pipeline succeeded.')
}

run().catch(console.error)
```

And use **async generators**:

```js
const pipeline = util.promisify(stream.pipeline)
const fs = require('fs')

async function run() {
	await pipeline(
		fs.createReadStream('lowercase.txt'),
		async function* (source) {
				source.setEncoding('utf8'); // Work with strings rather than `Buffer`s.
				for await (const chunk of source) {
					yield chunk.toUpperCase();
				}
			},
			fs.createWriteStream('uppercase.txt')
	)
	console.log('Pipeline succeeded.')
}

run().catch(console.error)
```

***





***



## Create

There are 2 ways to create stream:

```js
const { Readable } = require('stream')

// 1 - constructor
const myReadable = new Readable(opt)

// 2 - class extension
class myReadable extends Readable {
	constructor(opt){
		super(opt)
	}

	_read(size){}
}
```

We'll use the 2nd way. Either way, it takes a set of options. Some of them:

- `hightWaterMark` - max **buffer** size in bytes. Reading continues after the buffer is **empty** again (after `pipe`, `resume` or after processing the `data` event)
- `_read()` - protected method called implicitly until `highWaterMark` is reached. 
- `push()` - adds data to the **buffer**. Returns `false` if the buffer is full, `true` - otherwise. 

***


### Readable

Example: 

```js
const { Readable } = require('stream')

class Counter extends Readable {
	constructor(opt){
		super(opt)

		this._max = 100
		this._index = 0
	}

	_read(){
		this._index += 1
		if(this._index > this._max){
			this.push(null)
		} else {
			const buf = Buffer.from(`${this._index}`, 'utf8')
			console.log(`Added ${this._index}. Could be added? ${this.push(buf)}`)
		}
	}
}

const counter = new Counter({ highWaterMark: 2 })
console.log(`Received ${counter.read()}`)	// change this later
```

![](img/2020-09-23-16-39-34.png)

What happened here? 

1. In `new Counter({ highWaterMark: 2 })` we set the size of our inner buffer to **2 bytes**. Thus, it can store **2** of 1-byte (`utf8`) characters. 
2. `counter.read()` starts reading. 
   1. It writes '1' to the buffer.
   2. `Readable.push` - `return true` - can continue
   3. Repeat **1-2** for '2' 
   4. When the stream tries to write '3', `Readable.push()` will `return false` and the stream will wait until the **buffer** is empty. We don't have the buffer emptying yet, so the reading stops here. 

Let's add buffer emptying. Change the last line to this:

```js
counter.on('data', chunk => {
	console.log(`Received: ${chunk}`)
})
```

Now it counts from 1 to 100.

***


### Writable 

```js
const { Writable } = require('stream')

class myWritable extends Writable {
  	constructor(opt) {
    	super(opt)
  	}

  	_write(chunk, encoding, callback) {}
}
```

Similar to Readable:

- `_write(chunk, encoding, callback)` - called implicitly for writing a portion of data. Encoding is used is the data is `string`. 
- `highWaterMark` - max buffer size in bytes (16KB by default).