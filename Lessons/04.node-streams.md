# Streams

- [Streams](#streams)
	- [Intro](#intro)
	- [Stream Creation](#stream-creation)
		- [`fs.`**`createReadStream`**`(path[, options])`](#fscreatereadstreampath-options)
		- [`fs.`**`createWriteStream`**`(path[, options])`](#fscreatewritestreampath-options)
		- [`stream.`**`Readable.from`**`(iterable, [options])`](#streamreadablefromiterable-options)
	- [Reading Modes](#reading-modes)
		- [`readable.readableFlowing === null|false|true`](#readablereadableflowing--nullfalsetrue)
	- [Events](#events)
		- [`readable.on(`**`'data'`**`, callback)`](#readableondata-callback)
		- [`readable.on('`**`readable'`**`)`](#readableonreadable)
		- [`readable.on(`**`'end'`**`)`](#readableonend)
		- [`writable.on(`**`'close'`**`)`](#writableonclose)
		- [`writable.on(`**`'drain'`**`)`](#writableondrain)
		- [`writable.on(`**`'finish'`**`)`](#writableonfinish)
		- [`writable.on(`**`'pipe'`**`)`](#writableonpipe)
		- [`writable.on(`**`'unpipe'`**`)`](#writableonunpipe)
	- [Methods](#methods)
		- [`readable.`**`read`** `([size])`](#readableread-size)
		- [`readable.`**`pipe`** `(destination[, options])`](#readablepipe-destination-options)
		- [`stream.`**`pipeline`**`(source[, ...transforms], destination, callback)`](#streampipelinesource-transforms-destination-callback)
		- [`writable.`**`write`**`(chunk, encoding)`](#writablewritechunk-encoding)
		- [`writable.`**`end`**`([chunk[, encoding]][, callback])`](#writableendchunk-encoding-callback)
		- [`writable.`**`destroy`**`([error])`](#writabledestroyerror)
	- [Reading Streams](#reading-streams)
	- [Create](#create)
		- [Readable](#readable)
		- [Writable](#writable)

***

## Intro

Stream is a concept that allows **processing data by little pieces** or **chunks**.

There are 4 types of streams in node:

1. `Readable` - read (**req**)
2. `Writable` - write (**res**)
3. `Duplex` - both
4. `Transform` - a variation of duplex allowing to change data

***



## Stream Creation

### `fs.`**`createReadStream`**`(path[, options])`

![](img/2020-09-27-19-10-23.png)

***


### `fs.`**`createWriteStream`**`(path[, options])`

![](img/2020-09-27-19-19-34.png)

***


### `stream.`**`Readable.from`**`(iterable, [options])`

An utility method for creating readable streams out of **iterators**.

```js
const { Readable } = require('stream')

function * generate(){
	for(let i = 0; i < 1000; i++){
		yield i
	}
}

const r = Readable.from(generate())

r.on('readable', () => {
	let chunk
	while((chunk = r.read()) !== null){
		console.log(chunk)
	}
})

// r.on('data', console.log)
```

***



## Reading Modes

There are 2 modes for `Readable` streams: 

- **flowing mode**: data is read automatically and poured as quickly as possible.
- **paused mode**: `stream.read()` mush be called explicitly to read chunks of data from the stream. 

`Readable` streams begin in **paused** mode but can be switched to the **flowing** one by:

- adding the `data` event listener
- `stream.resume()`
- `stream.pipe()`

Switch back to **paused**:

- if there're no pipe destinations - `stream.pause()`
- if there're pipe destinations - remove them (`stream.unpipe()`)

**TLDR**: use the `readable` event and the `.read()` method to read the data and `pipeline()` to pipe it. 

***

### `readable.readableFlowing === null|false|true`

When `null` - no reading mechanism is provided, so data is not being read. 

***


## Events

**NB**: The default chunk size is **2 kB**.

### `readable.on(`**`'data'`**`, callback)`

The old way ot reading data. When the stream is in **flowing mode** (e.g. when the `data` listener is attached), chunks of data are passes as soon as possible. 

```js
const readable = getReadableStreamSomehow()
readable.on('data', (chunk) => {
	console.log(`Received ${chunk.length} bytes of data.`)
})
readable.on('end', () => {
	console.log('There will be no more data.')
})

/* ...
Received 65536 bytes of data.
Received 36595 bytes of data.
There will be no more data. */

```

Don't use it, use **`readable + .read()`** instead.

***


### `readable.on('`**`readable'`**`)`

The difference with `'data'` is that the stream is **not flowing**. Instead, we control the flow calling `stream.read()` explicitly.

Emitted when there is data available to be read from the stream (`stream.read()` will return **data** in this case). Will also be emitted at the end, just before the `end` event (`stream.read()` will return `null`).

```js
const readable = getReadableStreamSomehow()
readable.on('readable', function () {
	// There is some data to read now.
	let chunk

	while ((chunk = this.read()) !== null) {
		console.log(chunk)
	}
})
readable.on('end', function () {
	console.log('Stream finished')
})
```

If both `'readable'` and `'data'` are used at the same time, `'readable'` takes precedence in controlling the flow, i.e. `'data'` will be emitted only when `stream.read()` is called. The `readableFlowing` property would become `false`. If there are `'data'` listeners when `'readable'` is removed, the stream will start flowing, i.e. `'data'` events will be emitted without calling `.resume()`.

***


### `readable.on(`**`'end'`**`)`

Emitted when there is no more data to be consumed from the stream. Will only be emitted when all the data is **completely successfully consumed**. 

***


### `writable.on(`**`'close'`**`)`

Emitted when the stream and any of its underlying resources (e.g. a file descriptor) have been closed. The event indicates that **no more events will be emitted**, and **no further computation will occur**.

A `Writable` stream will always emit the `'close'` event if it is created with the `emitClose` option.

***


### `writable.on(`**`'drain'`**`)`

If a call to `stream.write(chunk)` returns `false`, the `'drain'` event will be emitted when it is appropriate to resume writing data to the stream.



TO ADD EXAMPLE


***


### `writable.on(`**`'finish'`**`)`

Emitted after the `stream.end()` method has been called, and all data has been flushed to the underlying system.



TO ADD EXAMPLE


***


### `writable.on(`**`'pipe'`**`)`

Emitted when the `stream.pipe()` method is called on a `Readable` stream, adding this `writable` to its set of destinations.


TO ADD EXAMPLE



***


### `writable.on(`**`'unpipe'`**`)`

Emitted when the `stream.unpipe()` method is called on a `Readable` stream, removing this `Writable` from its set of destinations.

Also emitted in case this `Writable` stream emits an **error** when a `Readable` stream pipes into it.


TO ADD EXAMPLE



***



## Methods

### `readable.`**`read`** `([size])`

Returns data from the internal buffer. If no data available, `return null`. 

If `readable.setEncoding()` was used to set the default encoding, then the chunk of data will be passed as a `string`, otherwise - as a `Buffer`.

`size` specifies how many bytes to read (should be **<= 1 GB**). If none is set, all of the data will be returned.

When reading a **large file** `.read()` may return `null`, having consumed all buffered content so far, but there is still more data to come **not yet buffered**. In this case a new `'readable'` event will be emitted when there is more data in the buffer. Finally the `'end'` event will be emitted when there is no more data to come. 

Therefore to read a file's whole contents from a readable, it is necessary to collect chunks across **multiple** `'readable'` events:

```js
const chunks = []

readable.on('readable', () => {
	let chunk
	while((chunk = readable.read()) !== null){
		chunks.push(chunk)
	}
})

readable.on('end', () => {
	const content = chunks.join('')
})
```

***


### `readable.`**`pipe`** `(destination[, options])`

Attaches a `Writable` stream to the `Readable`, causing it to switch automatically into **flowing mode** and push all of its data to the attached `Writable`.

It is also possible to attach multiple `Writable` streams to a single `Readable` one. 

Returns a reference to the **destination** stream making it possible to **chain** pipes if it is a `Duplex` or a `Transform` stream. 

```js
const r = fs.createReadStream('01. Dreaming Wide Awake.mp3')

const w1 = fs.createWriteStream('clone1.mp3')
const w2 = fs.createWriteStream('clone2.mp3')

r.pipe(w1)
r.pipe(w2)
```

***

**There is a problem with pipes**. If the `Readable` stream emits an **error** during processing, the `Writable` destination is **not closed** automatically: you'll have to manually close each stream in order to prevent memory leaks.

**Don't ever do this:**

```js
http.createServer((req, res) => {
	fs.createReadStream(__filename).pipe(res)
})
```

If the client closes connection before the transfer is over, or any other error occurs, the stream will **not end automatically**! It will hang forever.

The better way to pipe something is through the `pipeline`.

***


### `stream.`**`pipeline`**`(source[, ...transforms], destination, callback)`

A newer and better way to do pipes. 

```js
const { pipeline } = require('stream')

const server = http.createServer((req, res) => {
	// automatically destroy `res` if there's an error
	pipeline(fs.createReadStream(__filename), pipe(res), err => {
		if(err){
			console.log(err)
		}
	})
})
```

Example with archiving:

```js
const {	pipeline } = require('stream')
const fs = require('fs')
const zlib = require('zlib')

// Use the pipeline API to easily pipe a series of streams
// together and get notified when the pipeline is fully done.

// A pipeline to gzip a potentially huge tar file efficiently:

pipeline(
	fs.createReadStream('archive.tar'),
	zlib.createGzip(),
	fs.createWriteStream('archive.tar.gz'),
	(err) => {
		if (err) {
			console.error('Pipeline failed.', err)
		} else {
			console.log('Pipeline succeeded.')
		}
	}
)
```

You can primisify it as well:

```js
async function run() {
	await pipeline(
		fs.createReadStream('archive.tar'),
		zlib.createGzip(),
		fs.createWriteStream('archive.tar.gz')
	)
	console.log('Pipeline succeeded.')
}

run().catch(console.error)
```

And use **async generators**:

```js
const pipeline = util.promisify(stream.pipeline)
const fs = require('fs')

async function run() {
	await pipeline(
		fs.createReadStream('lowercase.txt'),
		async function* (source) {
				source.setEncoding('utf8'); // Work with strings rather than `Buffer`s.
				for await (const chunk of source) {
					yield chunk.toUpperCase();
				}
			},
			fs.createWriteStream('uppercase.txt')
	)
	console.log('Pipeline succeeded.')
}

run().catch(console.error)
```

***


### `writable.`**`write`**`(chunk[, encoding][, callback])`

**TLDR**

1. Use it to write to the stream.
2. Set `on.('error')` handler to track errors.
3. Stop writing as it begins to return `false` (buffer is full) and resume after `on.('drain')` (buffer is empty). 

Writes some data to the stream, and calls the supplied `callback` once the data has been fully handled. If an `error` occurs, the callback may or may not be called with the error as its first argument. To reliably detect write errors, add a listener for the `'error'` event. The `callback` is called **asynchronously** and **before** `'error'` is emitted.

Returns `true` if the internal buffer is less than the `highWaterMark`. Otherwise - `false`, further attempts to write data to the stream should stop until the `'drain'` event is emitted. While a stream is not draining, calls to `write()` will **buffer chunk**, and return `false`. 

Once all currently buffered chunks are drained (accepted for delivery by the operating system), the `'drain'` event will be emitted. 

It is recommended that once `write()` returns `false`, **no more chunks be written** until the `'drain'` event is emitted. While calling `write()` on a stream that is not draining is allowed, Node.js will buffer all written chunks until maximum memory usage occurs, at which point it will abort unconditionally. Even before it aborts, high memory usage will cause poor garbage collector performance and high RSS (which is not typically released back to the system, even after the memory is no longer required). Since TCP sockets may never drain if the remote peer does not read the data, writing a socket that is not draining may lead to a remotely exploitable vulnerability.


```js

```

***


### `writable.`**`end`**`([chunk[, encoding]][, callback])`

Signals that no more data will be written to the `Writable`.  The optional `chunk` and `encoding` arguments allow one final additional chunk of data to be written immediately before closing the stream.

***


### `writable.`**`destroy`**`([error])`

Destroy the stream. Optionally emit an `'error'` event, and emit a `'close'` event (unless `emitClose` is set to `false`).

This immediately destroys the stream. Use `end()` instead if data should flush before close, or wait for the `'drain'` event before destroying the stream.

***



## Reading Streams

There are 3 ways to read data from the stream: 

1. `.on('data')` - obsolete, don't use it
2. `.on('readable')`  + `read()` - relevant today
3. `for..await` - the best way in most cases, easy to use (you can `break` at any time and the stream will be closed automatically).

```js
const r = fs.createReadStream(path.join(__dirname, '01.DreamingWideAwake.mp3'))

run()

async function run(){
	let sumSize = 0
	for await(const chunk of r){
		console.log(chunk)	// 2 chunks of 64 kB
		sumSize += chunk.length
		if(sumSize >= 100000){
			break
		}
	}
}

// won't fire as reading was not completed
r.on('end', () => {
	console.log('end')
})

// will fire anyway
r.on('close', () => {
	console.log('close')
})
```



## Create

There are 2 ways to create stream:

```js
const { Readable } = require('stream')

// 1 - constructor
const myReadable = new Readable(opt)

// 2 - class extension
class myReadable extends Readable {
	constructor(opt){
		super(opt)
	}

	_read(size){}
}
```

We'll use the 2nd way. Either way, it takes a set of options. Some of them:

- `hightWaterMark` - max **buffer** size in bytes. Reading continues after the buffer is **empty** again (after `pipe`, `resume` or after processing the `data` event)
- `_read()` - protected method called implicitly until `highWaterMark` is reached. 
- `push()` - adds data to the **buffer**. Returns `false` if the buffer is full, `true` - otherwise. 

***


### Readable

Example: 

```js
const { Readable } = require('stream')

class Counter extends Readable {
	constructor(opt){
		super(opt)

		this._max = 100
		this._index = 0
	}

	_read(){
		this._index += 1
		if(this._index > this._max){
			this.push(null)
		} else {
			const buf = Buffer.from(`${this._index}`, 'utf8')
			console.log(`Added ${this._index}. Could be added? ${this.push(buf)}`)
		}
	}
}

const counter = new Counter({ highWaterMark: 2 })
console.log(`Received ${counter.read()}`)	// change this later
```

![](img/2020-09-23-16-39-34.png)

What happened here? 

1. In `new Counter({ highWaterMark: 2 })` we set the size of our inner buffer to **2 bytes**. Thus, it can store **2** of 1-byte (`utf8`) characters. 
2. `counter.read()` starts reading. 
   1. It writes '1' to the buffer.
   2. `Readable.push` - `return true` - can continue
   3. Repeat **1-2** for '2' 
   4. When the stream tries to write '3', `Readable.push()` will `return false` and the stream will wait until the **buffer** is empty. We don't have the buffer emptying yet, so the reading stops here. 

Let's add buffer emptying. Change the last line to this:

```js
counter.on('data', chunk => {
	console.log(`Received: ${chunk}`)
})
```

Now it counts from 1 to 100.

***


### Writable 

```js
const { Writable } = require('stream')

class myWritable extends Writable {
  	constructor(opt) {
    	super(opt)
  	}

  	_write(chunk, encoding, callback) {}
}
```

Similar to Readable:

- `_write(chunk, encoding, callback)` - called implicitly for writing a portion of data. Encoding is used is the data is `string`. 
- `highWaterMark` - max buffer size in bytes (16KB by default).